# agent/ImageAgent.py
import os
import cv2
import base64
import time
import subprocess
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI


class ImageAgent:
    def __init__(self, model="gpt-4o"):
        load_dotenv()

        self.llm = ChatOpenAI(
            model=model,
            api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0,
        )

        self.capture_path = "captured.jpg"

    def capture_image(self):
        """
        imagesnapì„ ì´ìš©í•´ macOS ì‹œìŠ¤í…œ ì¹´ë©”ë¼ì—ì„œ ì´ë¯¸ì§€ ìº¡ì²˜
        """
        print("ğŸ“¸ imagesnapìœ¼ë¡œ ì´ë¯¸ì§€ ìº¡ì²˜ ì¤‘...")
        subprocess.run(["imagesnap", "-w", "2", self.capture_path])
        print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ë¨: {self.capture_path}")
        image = cv2.imread(self.capture_path)
        if image is None:
            raise RuntimeError("âŒ ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨ (imagesnap ê²°ê³¼ í™•ì¸ í•„ìš”)")
        return image

    def image_to_base64(self, image):
        # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
        cv2.imwrite("original.jpg", image)

        # ì›ë³¸ ê·¸ëŒ€ë¡œ ì¸ì½”ë”©
        _, buffer = cv2.imencode(".jpg", image)
        return base64.b64encode(buffer).decode("utf-8")

    def classify(self) -> str:
        # image = self.capture_image()
        image_b64 = self.image_to_base64(image)

        messages = [
            HumanMessage(
                content=[
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"},
                    },
                    {
                        "type": "text",
                        "text": "ì´ ì´ë¯¸ì§€ì— ìˆëŠ” M&M'sì˜ ìƒ‰ê¹”ì„ í•œ ë‹¨ì–´ë¡œë§Œ ëŒ€ë‹µí•´ì¤˜. (ì˜ˆ: red, green, yellow, blue).whiteë‚˜ black, gray ê°™ì€ ìƒ‰ì€ ì—†ì–´. ê·¸ë¦¼ìì˜ ì˜í–¥ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ê°ì•ˆí•´ì¤˜. ê·¸ë¦¬ê³  ì˜¤ë¡œì§€ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•´ì¤˜. ë‹¨ì–´ ë§ê³  ë‹¤ë¥¸ ë¬¸ì¥ ë¬¸ì¥ë¶€í˜¸ëŠ” ë„£ì§€ë§ˆ. ìµœëŒ€í•œ ìƒ‰ì„ êµ¬ë¶„í•˜ë˜, ì•„ë¬´ê²ƒë„ ì—†ë‹¤ë©´ unknownì„ ì¶œë ¥í•´ì¤˜.",
                    },
                ]
            )
        ]

        print("ğŸ§  GPT-4oì—ê²Œ ë¶„ë¥˜ ìš”ì²­ ì¤‘...")
        response = self.llm.invoke(messages)
        color = response.content.strip().lower()
        print(response)

        valid_colors = ["red", "green", "yellow", "blue", "orange", "brown"]
        if color in valid_colors:
            return color
        else:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ìƒ‰ìƒ: {color}")
            return "unknown"

    def release_camera(self):
        pass  # imagesnapì€ ì¹´ë©”ë¼ ì ìœ ê°€ ì—†ê¸° ë•Œë¬¸ì— ë¬´ì‹œ
