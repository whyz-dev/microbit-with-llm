# agent/ImageAgent.py
import os
import cv2
import base64
import time
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI


class ImageAgent:
    def __init__(self, model="gpt-4o"):
        load_dotenv()

        self.llm = ChatOpenAI(
            model=model,
            api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0,
        )

        # ì¹´ë©”ë¼ ì—°ê²°
        print("ğŸ“¸ ì¹´ë©”ë¼ ì—°ê²° ì‹œë„ ì¤‘...")
        self.cap = cv2.VideoCapture(0)
        retry = 0
        while not self.cap.isOpened():
            retry += 1
            print(f"â³ ì¹´ë©”ë¼ ì¤€ë¹„ ì¤‘... ì¬ì‹œë„ {retry}")
            time.sleep(1)
            self.cap = cv2.VideoCapture(0)

        print("âœ… ì¹´ë©”ë¼ ì—°ê²° ì™„ë£Œ!")

    def capture_image(self):
        print("ğŸ–¼ ì´ë¯¸ì§€ ìº¡ì²˜ ì¤‘...")
        for _ in range(10):
            ret, frame = self.cap.read()
            if ret:
                print("âœ… ì´ë¯¸ì§€ ìº¡ì²˜ ì„±ê³µ!")
                return frame
            print("âš ï¸ ì‹¤íŒ¨. ì¬ì‹œë„...")
            time.sleep(0.5)
        raise RuntimeError("âŒ ì´ë¯¸ì§€ ìº¡ì²˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    def image_to_base64(self, image):
        _, buffer = cv2.imencode(".jpg", image)
        return base64.b64encode(buffer).decode("utf-8")

    def classify(self) -> str:
        image = self.capture_image()
        image_b64 = self.image_to_base64(image)

        messages = [
            HumanMessage(
                content=[
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"},
                    },
                    {
                        "type": "text",
                        "text": "ì´ ì´ë¯¸ì§€ì— ìˆëŠ” M&M'sì˜ ìƒ‰ê¹”ì„ í•œ ë‹¨ì–´ë¡œë§Œ ëŒ€ë‹µí•´ì¤˜. (ì˜ˆ: red, green, yellow, blue).whiteë‚˜ black, gray ê°™ì€ ìƒ‰ì€ ì—†ì–´. ê·¸ë¦¼ìì˜ ì˜í–¥ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ê°ì•ˆí•´ì¤˜. ê·¸ë¦¬ê³  ì˜¤ë¡œì§€ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•´ì¤˜. ë‹¨ì–´ ë§ê³  ë‹¤ë¥¸ ë¬¸ì¥ ë¬¸ì¥ë¶€í˜¸ëŠ” ë„£ì§€ë§ˆ. ìµœëŒ€í•œ ìƒ‰ì„ êµ¬ë¶„í•˜ë˜, ì•„ë¬´ê²ƒë„ ì—†ë‹¤ë©´ unknownì„ ì¶œë ¥í•´ì¤˜.",
                    },
                ]
            )
        ]

        print("ğŸ§  GPT-4oì—ê²Œ ë¶„ë¥˜ ìš”ì²­ ì¤‘...")
        response = self.llm.invoke(messages)
        color = response.content.strip().lower()

        # ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸ ê²€ì¦ (ì˜ˆ: red, green, yellow, blue)
        valid_colors = ["red", "green", "yellow", "blue", "orange", "brown"]
        if color in valid_colors:
            return color
        else:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ìƒ‰ìƒ: {color}")
            return "unknown"

    def release_camera(self):
        if self.cap.isOpened():
            self.cap.release()
            print("ğŸ“· ì¹´ë©”ë¼ ì—°ê²° ì¢…ë£Œë¨.")
